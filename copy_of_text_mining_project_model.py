# -*- coding: utf-8 -*-
"""Copy of text_mining_project_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vHVoXOWynp6WcNU1gUN_HjkhlJuhDP4t

# Import libs
"""

# !nvidia-smi
# !rm -r results/ logs/ wandb/

# !pip install --quiet transformers
# !pip install --quiet datasets
# !pip install --quiet wandb
# !pip install --quiet underthesea

# from google.colab import drive
# drive.mount('/content/drive')

import pandas as pd
import numpy as np
import os

from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, AutoModel
from datasets import Dataset
#from sklearn.model_selection import train_test_split
from datasets import load_metric
# import wandb
import re 
from underthesea import word_tokenize

# # nghethuat
# !gdown --id 1uE2QHeJrv_eM8pTAbCpYkZtq4v98USxg
# !gdown --id 1terNR0xZtBa5IAipfxO88XFwJdZjc0_t
# !gdown --id 1Bva3f0rBNmI5AXmBQI1gHTs9tNqK37sI
# # baochi
# !gdown --id 1Ot_mNsTbvi7MQOEkZXE4EgPq2MXcMlA4
#
# # sinhhoat
# !gdown --id 1-6Kp2CepcRbjwmxCkh4y9dLsF9tTYXQo
#
# # khoa hoc
# !gdown --id 1HqWsIqOfhPsZzYqePrys-sIOK2FSIk6W
#
# # hanh chinh
# !gdown --id 1-qSyaPb3OZCI57pw37Sg7GMILHMMxZpv
#
# #chinh luan
# !gdown --id 1yXEglRsGdkqn5hjuVYKh2OZ9UkSKsP4e
#
# """# Read csvs"""
#
# df = []
# df = pd.DataFrame({'text': [], 'label': []})
# for filename in os.listdir():
#     if filename[-3:] == 'csv':
#         print(filename)
#         temp = pd.read_csv(filename, encoding = 'utf-8')
#         if 'Type' in temp.columns:
#             temp = temp.rename({'Text': 'text', 'Type': 'label'}, axis = 1)
#         df = pd.concat([df, temp])


# df = df.drop([0], axis = 1, errors = 'ignore')
# df = df.dropna(subset = ['label', 'text'])
# df = df.drop(['Unnamed: 0'], axis = 1)
# print(df.shape)
# df = df.drop_duplicates(subset=['text']).reset_index(drop = True)
# print(df.shape)
# display(df)
#
# df.label.value_counts()
#
# seed = 15
# train_df, test_df = train_test_split(df, test_size=0.3, stratify = df['label'], random_state = seed)
# train_df, val_df = train_test_split(train_df, test_size=0.2, stratify = train_df['label'], random_state = seed)
#
# print(train_df.shape, val_df.shape, test_df.shape)

"""# Hyperparameters"""

# setup dataset & tokenizer
num_labels = 6
max_seq_len = 256
overwrite_output_dir = True

# Training 
epochs = 3
train_batch_size = 16
val_batch_size = 16
lr = 1e-5
logging_steps = 10

step_per_epoch = int(len(train_df) / train_batch_size)
warmup_steps = int(step_per_epoch * 2/3)    #takes 2/3 first epoch for warming up

weight_decay = 0.28
gradient_accumulation_steps = 3
eval_steps = 50
# output_dir = './results/'
# logging_dir = './logs/'

"""# Load Model + tokenizer"""

tokenizer = AutoTokenizer.from_pretrained("vinai/phobert-base")
#model = AutoModelForSequenceClassification.from_pretrained("vinai/phobert-base", num_labels= num_labels)

#!cp -r ./sample_data /content/drive/MyDrive

"""# Create custom Dataset + Padding"""

# def create_dataset(df):
#     df['text'] = df['text'].astype(str)
#     df['label'] = df['label'].replace(['bao chi','sinh hoat','nghe thuat', 'khoa hoc', 'HC','chinh luan'],[0,1,2,3,4,5])
#     dataset = {'text': df['text'], 'label': df['label']}
#     dataset = Dataset.from_dict(dataset)
#     return dataset
#
# train_dataset = create_dataset(train_df)
# val_dataset = create_dataset(val_df)
# test_dataset = create_dataset(test_df)
#
# print(train_dataset, val_dataset, test_dataset)

def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True, max_length = max_seq_len)

# train_dataset = train_dataset.map(tokenize_function, batched=True)
# val_dataset = val_dataset.map(tokenize_function, batched=True)
# test_dataset = test_dataset.map(tokenize_function, batched=True)
#
# small_train_dataset = train_dataset.select(range(100))
# small_val_dataset = train_dataset.select(range(100, 200))

"""# Metrics"""

# from datasets import list_metrics
# metrics_list = list_metrics()
# metrics_list
#
# metric = load_metric("accuracy")
#
# def compute_metrics(eval_pred):
#     logits, labels = eval_pred
#     predictions = np.argmax(logits, axis=-1)
#     return metric.compute(predictions=predictions, references=labels)

"""# Modeling

## Train
"""

# Commented out IPython magic to ensure Python compatibility.
# %env WANDB_WATCH=all

# training_args = TrainingArguments(
#     output_dir=                     output_dir,
#     num_train_epochs=               epochs,
#     per_device_train_batch_size=    train_batch_size,
#     per_device_eval_batch_size=     val_batch_size,
#     warmup_steps=                   warmup_steps,
#     weight_decay=                   weight_decay,
#     overwrite_output_dir =          overwrite_output_dir,
#     logging_dir=                    logging_dir,
#     evaluation_strategy=            "steps",
#     eval_steps=                     eval_steps,
#
#     report_to=                      'wandb',
#     run_name=                       'text_mining bert',
#     logging_steps =                 logging_steps,
#     save_steps=                     step_per_epoch,
#     gradient_accumulation_steps=    gradient_accumulation_steps
#
#
# )
#
# trainer = Trainer(
#     model=model,                         # the instantiated ğŸ¤— Transformers model to be trained
#     args=training_args,                  # training arguments, defined above
#     train_dataset=train_dataset,         # training dataset
#     eval_dataset=val_dataset,            # evaluation dataset
#     compute_metrics = compute_metrics
# )
#
# trainer.train()

"""## Evaluation"""

# trainer.evaluate()
# wandb.finish()

"""## Final test"""

# from sklearn.metrics import classification_report
#
# preds = trainer.predict(test_dataset)
# y_preds = np.argmax(np.array(preds[0]), axis = 1)
#
# preds
#
# y_preds
#
# from sklearn.metrics import classification_report, confusion_matrix
# import matplotlib.pyplot as plt
# import seaborn as sns
#
# plt.rcParams["figure.figsize"] = (15,10)
# plt.rcParams["figure.figsize"] = (15,10)

#
# print(classification_report(test_df['label'], y_preds))
# sns.heatmap(confusion_matrix(test_df['label'], y_preds), annot = True, fmt='g')
#
# """# Save model"""
#
# trainer.model.save_pretrained("final_model_6C")
#
# !cp -r ./final_model_6C /content/drive/MyDrive

"""# Inference

## Load model
"""

# !gdown --id 1V4jhECU57OP8RCaThVbspJG-zP2jcyxt
#
# !mkdir final_model_6C
# !unzip model_6_PCNN -d final_model_6C

model_load = AutoModelForSequenceClassification.from_pretrained("final_model_6C")

"""## Trainer for model"""

# arguments for Trainer
inf_batch_size = 64

test_args = TrainingArguments(
    output_dir = "tmp_trainer",
    do_train = False,
    do_predict = True,
    per_device_eval_batch_size = inf_batch_size    
)

trainer = Trainer(
    model=model_load, 
    args = test_args                       
)

"""## Predict"""

from scipy.special import softmax

def preprocess(txt):
    def clean_html(raw_html):
        CLEANR = re.compile('<.*?>')    
        cleantext = re.sub(CLEANR, '', raw_html)
        return cleantext

    def remove_special_chars(txt):
        regex = r"[^,.!\"\'(...);\w\s]"
        return re.sub(regex, '', txt)

    def normalize_unicode(txt):
        def loaddicchar():
            dic = {}
            char1252 = 'aÌ€|aÌ|aÌ‰|aÌƒ|aÌ£|Ã¢Ì€|Ã¢Ì|Ã¢Ì‰|Ã¢Ìƒ|Ã¢Ì£|ÄƒÌ€|ÄƒÌ|ÄƒÌ‰|ÄƒÌƒ|ÄƒÌ£|eÌ€|eÌ|eÌ‰|eÌƒ|eÌ£|ÃªÌ€|ÃªÌ|ÃªÌ‰|ÃªÌƒ|ÃªÌ£|iÌ€|iÌ|iÌ‰|iÌƒ|iÌ£|oÌ€|oÌ|oÌ‰|oÌƒ|oÌ£|Ã´Ì€|Ã´Ì|Ã´Ì‰|Ã´Ìƒ|Ã´Ì£|Æ¡Ì€|Æ¡Ì|Æ¡Ì‰|Æ¡Ìƒ|Æ¡Ì£|uÌ€|uÌ|uÌ‰|uÌƒ|uÌ£|Æ°Ì€|Æ°Ì|Æ°Ì‰|Æ°Ìƒ|Æ°Ì£|yÌ€|yÌ|yÌ‰|yÌƒ|yÌ£|AÌ€|AÌ|AÌ‰|AÌƒ|AÌ£|Ã‚Ì€|Ã‚Ì|Ã‚Ì‰|Ã‚Ìƒ|Ã‚Ì£|Ä‚Ì€|Ä‚Ì|Ä‚Ì‰|Ä‚Ìƒ|Ä‚Ì£|EÌ€|EÌ|EÌ‰|EÌƒ|EÌ£|ÃŠÌ€|ÃŠÌ|ÃŠÌ‰|ÃŠÌƒ|ÃŠÌ£|IÌ€|IÌ|IÌ‰|IÌƒ|IÌ£|OÌ€|OÌ|OÌ‰|OÌƒ|OÌ£|Ã”Ì€|Ã”Ì|Ã”Ì‰|Ã”Ìƒ|Ã”Ì£|Æ Ì€|Æ Ì|Æ Ì‰|Æ Ìƒ|Æ Ì£|UÌ€|UÌ|UÌ‰|UÌƒ|UÌ£|Æ¯Ì€|Æ¯Ì|Æ¯Ì‰|Æ¯Ìƒ|Æ¯Ì£|YÌ€|YÌ|YÌ‰|YÌƒ|YÌ£'.split(
                '|')
            charutf8 = "Ã |Ã¡|áº£|Ã£|áº¡|áº§|áº¥|áº©|áº«|áº­|áº±|áº¯|áº³|áºµ|áº·|Ã¨|Ã©|áº»|áº½|áº¹|á»|áº¿|á»ƒ|á»…|á»‡|Ã¬|Ã­|á»‰|Ä©|á»‹|Ã²|Ã³|á»|Ãµ|á»|á»“|á»‘|á»•|á»—|á»™|á»|á»›|á»Ÿ|á»¡|á»£|Ã¹|Ãº|á»§|Å©|á»¥|á»«|á»©|á»­|á»¯|á»±|á»³|Ã½|á»·|á»¹|á»µ|Ã€|Ã|áº¢|Ãƒ|áº |áº¦|áº¤|áº¨|áºª|áº¬|áº°|áº®|áº²|áº´|áº¶|Ãˆ|Ã‰|áºº|áº¼|áº¸|á»€|áº¾|á»‚|á»„|á»†|ÃŒ|Ã|á»ˆ|Ä¨|á»Š|Ã’|Ã“|á»|Ã•|á»Œ|á»’|á»|á»”|á»–|á»˜|á»œ|á»š|á»|á» |á»¢|Ã™|Ãš|á»¦|Å¨|á»¤|á»ª|á»¨|á»¬|á»®|á»°|á»²|Ã|á»¶|á»¸|á»´".split(
                '|')
            for i in range(len(char1252)):
                dic[char1252[i]] = charutf8[i]
            return dic
        
        
        dicchar = loaddicchar()
        

        return re.sub(r'aÌ€|aÌ|aÌ‰|aÌƒ|aÌ£|Ã¢Ì€|Ã¢Ì|Ã¢Ì‰|Ã¢Ìƒ|Ã¢Ì£|ÄƒÌ€|ÄƒÌ|ÄƒÌ‰|ÄƒÌƒ|ÄƒÌ£|eÌ€|eÌ|eÌ‰|eÌƒ|eÌ£|ÃªÌ€|ÃªÌ|ÃªÌ‰|ÃªÌƒ|ÃªÌ£|iÌ€|iÌ|iÌ‰|iÌƒ|iÌ£|oÌ€|oÌ|oÌ‰|oÌƒ|oÌ£|Ã´Ì€|Ã´Ì|Ã´Ì‰|Ã´Ìƒ|Ã´Ì£|Æ¡Ì€|Æ¡Ì|Æ¡Ì‰|Æ¡Ìƒ|Æ¡Ì£|uÌ€|uÌ|uÌ‰|uÌƒ|uÌ£|Æ°Ì€|Æ°Ì|Æ°Ì‰|Æ°Ìƒ|Æ°Ì£|yÌ€|yÌ|yÌ‰|yÌƒ|yÌ£|AÌ€|AÌ|AÌ‰|AÌƒ|AÌ£|Ã‚Ì€|Ã‚Ì|Ã‚Ì‰|Ã‚Ìƒ|Ã‚Ì£|Ä‚Ì€|Ä‚Ì|Ä‚Ì‰|Ä‚Ìƒ|Ä‚Ì£|EÌ€|EÌ|EÌ‰|EÌƒ|EÌ£|ÃŠÌ€|ÃŠÌ|ÃŠÌ‰|ÃŠÌƒ|ÃŠÌ£|IÌ€|IÌ|IÌ‰|IÌƒ|IÌ£|OÌ€|OÌ|OÌ‰|OÌƒ|OÌ£|Ã”Ì€|Ã”Ì|Ã”Ì‰|Ã”Ìƒ|Ã”Ì£|Æ Ì€|Æ Ì|Æ Ì‰|Æ Ìƒ|Æ Ì£|UÌ€|UÌ|UÌ‰|UÌƒ|UÌ£|Æ¯Ì€|Æ¯Ì|Æ¯Ì‰|Æ¯Ìƒ|Æ¯Ì£|YÌ€|YÌ|YÌ‰|YÌƒ|YÌ£',
                    lambda x: dicchar[x.group()], txt)

    def tokenize(txt):
        return word_tokenize(txt, format="text")

    temp = txt
    temp = clean_html(temp)
    temp = remove_special_chars(temp)
    temp = normalize_unicode(temp)
    temp = temp.strip()

    temp = tokenize(temp)
    return temp

def predict(trainer, text):
    def create_dataset_inference(text):
        df = pd.DataFrame({'text': text})
        df['text'] = df['text'].astype(str)
        df['text'] = df['text'].apply(preprocess)

        dataset = {'text': df['text']}
        dataset = Dataset.from_dict(dataset)
        return dataset

    dataset = create_dataset_inference(text)
    dataset = dataset.map(tokenize_function, batched=True)
    # predict
    preds = np.array(trainer.predict(dataset)[0])

    # convert to label
    label = np.argmax(preds, axis = 1)
    prob = np.max(softmax(preds, axis = 1), axis = 1)
    
    pred_df = pd.DataFrame({'text': text, 'label': label, 'probability': prob})
    pred_df['label'] = pred_df['label'].replace([0,1,2,3,4,5], ['bao chi','sinh hoat','nghe thuat', 'khoa hoc', 'hanh chinh','chÃ­nh luáº­n'])
    return pred_df

# text = ["Jessica Ã , mÃ y Ä‘Ã£ ngá»§ vá»›i tháº§y giÃ¡o Ä‘Ãºng khÃ´ng",
#         "Em Æ¡i lÃ¢u Ä‘Ã i tÃ¬nh Ã¡i Ä‘Ã³ , cháº¯c khÃ´ng cÃ³ trÃªn tráº§n gian",
#         "Viá»‡t Nam lÃ  nÆ°á»›c cÃ³ tiá»m nÄƒng phÃ¡t triá»ƒn náº¥m Äƒn vÃ  náº¥m dÆ°á»£c liá»‡u. Theo thá»‘ng_kÃª cá»§a Tá»• chá»©c NÃ´ng lÆ°Æ¡ng tháº¿_giá»›i, sáº£n_xuáº¥t náº¥m á»Ÿ Viá»‡t_Nam Ä‘Æ°á»£c xáº¿p hÃ ng thá»© chÃ­n trong khu_vá»±c. Sáº£n_lÆ°á»£ng náº¥m Ä‘áº¡t khoáº£ng 250 nghÃ¬n táº¥n náº¥m tÆ°Æ¡i/nÄƒm, tháº¥p hÆ¡n ráº¥t nhiá»u so vá»›i tiá»mnÄƒng vÃ  cÃ¡c nÆ°á»›c trong khu vá»±c",
#         "Nháº±m tÄƒng cÆ°á»ng miá»…n dá»‹ch phÃ²ng COVID-19 cho nhá»¯ng ngÆ°á»i Ä‘Ã£ Ä‘Æ°á»£c tiÃªm chá»§ng Ä‘á»§ liá»u cÆ¡ báº£n, Ban Chá»‰ Ä‘áº¡o phÃ²ng, chá»‘ng dá»‹ch COVID-19 ThÃ nh phá»‘ xÃ¢y dá»±ng Káº¿ hoáº¡ch tá»• chá»©c tiÃªm váº¯c xin phÃ²ng COVID-19 liá»u bá»• sung vÃ  nháº¯c láº¡i táº¡i ThÃ nh phá»‘ Há»“ ChÃ­ Minh, cá»¥ thá»ƒ nhÆ° sau",
#         "Má»›i Ä‘Ã¢y, ca sÄ© ÄÃ m VÄ©nh HÆ°ng Ä‘Ã£ báº¥t ngá» chia sáº» nhá»¯ng hÃ¬nh áº£nh khÃ¡ â€œmÃ¡t máº»â€ cá»§a mÃ¬nh trÃªn trang cÃ¡ nhÃ¢n. Trong nhá»¯ng bá»©c áº£nh, Mr ÄÃ m thá»ƒ hiá»‡n sá»± thÆ° giÃ£n khi táº¯m há»“ bÆ¡i ngay trÃªn sÃ¢n thÆ°á»£ng nhÃ  mÃ¬nh, Ä‘Ã¹a nghá»‹ch cÃ¹ng chim bá»“ cÃ¢u vÃ  chÃº cÃºn nhá», Ä‘áº·c biá»‡t lÃ  khoe cáº£ hÃ¬nh xÄƒm trÃªn ngá»±c tráº§n."
#         ]
#
# pd.set_option('display.max_colwidth', None)
# predict(trainer, text)

